# ğŸ¤– Hive Mind - Assistant Vocal DistribuÃ©

## ğŸ“‹ Vue d'ensemble du projet

**Hive Mind** est un systÃ¨me d'assistant vocal distribuÃ© avec une architecture client-serveur oÃ¹ :
- **Serveur "Hive Mind"** : Cerveau central hÃ©bergeant l'IA et les intÃ©grations
- **Clients** : Interfaces vocales lÃ©gÃ¨res (PC, puis robots embarquÃ©s) partageant le mÃªme contexte conversationnel
- **Mode dÃ©gradÃ©** : FonctionnalitÃ©s offline pour commandes basiques

---

## ğŸ¯ Objectifs et contraintes

### Performances
- **Latence maximale** : 3 secondes (question â†’ dÃ©but de rÃ©ponse)
- **IA** : CPU-only, lightweight (pas de GPU dÃ©diÃ©)
- **MatÃ©riel cible** : Du PC au matÃ©riel embarquÃ© (ESP32/Raspberry Pi Zero)

### FonctionnalitÃ©s
#### Mode connectÃ© (serveur disponible)
- ComprÃ©hension vocale (STT) en franÃ§ais
- RÃ©ponses conversationnelles via IA
- IntÃ©grations tierces (mÃ©tÃ©o, Google Calendar, etc.)
- Actions multi-appareils
- SynthÃ¨se vocale (TTS)

#### Mode dÃ©gradÃ© (serveur indisponible)
- Heure et date
- ChronomÃ¨tre
- Minuteur
- Indicateur visuel (yeux jaunes vs blancs)

### Wake Word
- Activation vocale configurable : "Hey [nom du robot]"
- Par dÃ©faut paramÃ©trable

---

## ğŸ—ï¸ Architecture technique

### Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVEUR PYTHON (Hive Mind)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  FastAPI Core                                           â”‚ â”‚
â”‚  â”‚  â€¢ WebSocket bidirectionnel                             â”‚ â”‚
â”‚  â”‚  â€¢ Context Manager (Redis)                              â”‚ â”‚
â”‚  â”‚  â€¢ Plugin Loader dynamique                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  LLM Engine                                             â”‚ â”‚
â”‚  â”‚  â€¢ Phi-3-mini-4k (3.8B params, Q4_K_M, ~2.3GB)         â”‚ â”‚
â”‚  â”‚  â€¢ llama-cpp-python (optimisations CPU AVX2)            â”‚ â”‚
â”‚  â”‚  â€¢ Streaming responses                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Plugin System                                          â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ weather/        (OpenWeatherMap API)               â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ calendar/       (Google Calendar OAuth2)           â”‚ â”‚
â”‚  â”‚  â””â”€â”€ [plugin-template]/                                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–²
                          â”‚ WebSocket (Protocol JSON)
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT RUST                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Audio Pipeline                                        â”‚â”‚
â”‚  â”‚  â€¢ cpal (capture microphone cross-platform)            â”‚â”‚
â”‚  â”‚  â€¢ whisper.cpp (STT local, modÃ¨le base 74MB)          â”‚â”‚
â”‚  â”‚  â€¢ piper (TTS local haute qualitÃ©)                     â”‚â”‚
â”‚  â”‚  â€¢ porcupine (wake word "Hey [nom]")                   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Core Logic                                            â”‚â”‚
â”‚  â”‚  â€¢ State Machine (connected/degraded)                  â”‚â”‚
â”‚  â”‚  â€¢ WebSocket client (tokio-tungstenite)                â”‚â”‚
â”‚  â”‚  â€¢ Local commands handler (time, timer, chrono)        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  UI Layer                                              â”‚â”‚
â”‚  â”‚  â€¢ Desktop: egui (fenÃªtre avec yeux animÃ©s)            â”‚â”‚
â”‚  â”‚  â€¢ Embedded: embedded-graphics (futur)                 â”‚â”‚
â”‚  â”‚  â€¢ Eye colors: blanc=connectÃ©, jaune=dÃ©gradÃ©           â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RÃ©seau
- **Phase 1** : LAN uniquement (clients et serveur sur mÃªme rÃ©seau local)
- **Phase 2+** : PossibilitÃ© d'accÃ¨s Internet via reverse proxy (optionnel)

### Contexte conversationnel
- **Scope** : PartagÃ© entre tous les clients (single user)
- **Persistance** : Redis (7 jours rolling window)
- **Nettoyage** : Automatique chaque semaine
- **Format** : Historique messages + mÃ©tadonnÃ©es (timestamp, client_id, intent)

---

## ğŸ› ï¸ Stack technologique

### Serveur Python

#### DÃ©pendances principales
```python
fastapi==0.109.0              # Framework web async
uvicorn[standard]==0.27.0     # ASGI server
llama-cpp-python==0.2.27      # Bindings llama.cpp optimisÃ© CPU
pydantic==2.5.0               # Validation + schÃ©mas
redis==5.0.1                  # Context store
httpx==0.26.0                 # HTTP client async pour APIs
google-auth==2.27.0           # Google Calendar OAuth2
python-dotenv==1.0.0          # Config environment
```

#### ModÃ¨le IA : Phi-3-mini
- **ModÃ¨le** : `microsoft/Phi-3-mini-4k-instruct-gguf`
- **Quantification** : Q4_K_M (2.3 GB)
- **RAM nÃ©cessaire** : 4-6 GB
- **Latence CPU** : ~1-2s pour 50 tokens (8 threads)
- **Raisons du choix** :
  - OptimisÃ© pour edge computing
  - Excellent instruction-following
  - Support conversations multi-tours
  - Performance CPU exceptionnelle

#### HÃ©bergement
- **Infrastructure** : Proxmox (serveur domestique)
- **DÃ©ploiement** : Docker Compose
  - Container serveur Python
  - Container Redis
  - Volume persistant pour modÃ¨les IA
- **RÃ©seau** : Bridge Docker avec exposition port WebSocket

#### Architecture serveur

```
serveur/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm.py                 # Wrapper llama-cpp-python
â”‚   â”œâ”€â”€ context.py             # Context manager (Redis)
â”‚   â”œâ”€â”€ websocket.py           # Handler WebSocket
â”‚   â”œâ”€â”€ plugin.py              # Base class Plugin
â”‚   â””â”€â”€ plugin_loader.py       # Dynamic plugin loading
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ weather/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ plugin.json        # Manifest
â”‚   â”‚   â”œâ”€â”€ handler.py         # Logique mÃ©tier
â”‚   â”‚   â””â”€â”€ tests/
â”‚   â”œâ”€â”€ calendar/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ _template/             # Template pour nouveaux plugins
â”‚       â”œâ”€â”€ plugin.json
â”‚       â””â”€â”€ handler.py
â”œâ”€â”€ main.py                    # Point d'entrÃ©e FastAPI
â”œâ”€â”€ config.py                  # Configuration centralisÃ©e
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ tests/
    â””â”€â”€ ...
```

---

### Client Rust

#### DÃ©pendances Cargo
```toml
[dependencies]
# Async runtime
tokio = { version = "1.35", features = ["full"] }
tokio-tungstenite = "0.21"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Audio pipeline
cpal = "0.15"                  # Cross-platform audio I/O
whisper-rs = "0.10"            # Bindings whisper.cpp
hound = "3.5"                  # WAV encoding/decoding

# Wake word detection
pv-porcupine = "2.2"           # Picovoice wake word

# TTS
tts = "0.26"                   # Rust TTS wrapper (espeak backend)
# Note: Migration vers piper prÃ©vue en Phase 2

# UI Desktop
eframe = "0.25"                # egui framework
egui = "0.25"

# Logging & errors
tracing = "0.1"
tracing-subscriber = "0.3"
anyhow = "1.0"
thiserror = "1.0"

# Configuration
config = "0.13"
dotenv = "0.15"
```

#### Architecture client

```
client/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs                # Entry point
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ audio.rs           # Audio pipeline (STT/TTS/wake word)
â”‚   â”‚   â”œâ”€â”€ websocket.rs       # Client WebSocket
â”‚   â”‚   â”œâ”€â”€ state.rs           # State machine (connected/degraded)
â”‚   â”‚   â””â”€â”€ commands.rs        # Local commands (time, timer, etc.)
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ desktop.rs         # Desktop UI (egui)
â”‚   â”‚   â”œâ”€â”€ eyes.rs            # Eye animation component
â”‚   â”‚   â””â”€â”€ embedded.rs        # Futur: embedded displays
â”‚   â”œâ”€â”€ platform/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ desktop.rs         # Platform-specific (Windows/Linux)
â”‚   â”‚   â””â”€â”€ embedded.rs        # Futur: ESP32/RasPi
â”‚   â””â”€â”€ config.rs              # Configuration
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ whisper-base.bin       # ModÃ¨le Whisper (74MB)
â”‚   â”œâ”€â”€ porcupine_params.pv    # Wake word model
â”‚   â””â”€â”€ config.toml
â”œâ”€â”€ Cargo.toml
â””â”€â”€ tests/
    â””â”€â”€ ...
```

---

## ğŸ”Œ SystÃ¨me de plugins (Serveur)

### Convention standardisÃ©e

Chaque plugin doit respecter :

#### 1. Structure de dossier
```
plugins/<nom_plugin>/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ plugin.json           # Manifest (mÃ©tadonnÃ©es)
â”œâ”€â”€ handler.py            # Classe hÃ©ritant de Plugin
â””â”€â”€ tests/
    â””â”€â”€ test_<nom>.py
```

#### 2. Manifest `plugin.json`
```json
{
  "name": "weather",
  "version": "1.0.0",
  "description": "RÃ©cupÃ¨re les informations mÃ©tÃ©orologiques",
  "triggers": ["mÃ©tÃ©o", "weather", "tempÃ©rature", "temps qu'il fait"],
  "intents": ["get_weather", "get_forecast"],
  "config": {
    "api_key_env": "OPENWEATHER_API_KEY",
    "default_location": "Belfort, FR"
  },
  "dependencies": ["httpx"],
  "enabled": true
}
```

#### 3. Interface Python (base class)
```python
from abc import ABC, abstractmethod
from typing import Any, Dict, Optional

class Plugin(ABC):
    """Base class pour tous les plugins Hive Mind"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
    
    @abstractmethod
    async def execute(self, intent: str, params: Dict) -> Dict:
        """
        ExÃ©cute l'action du plugin
        
        Args:
            intent: Nom de l'intent (ex: "get_weather")
            params: ParamÃ¨tres extraits par l'IA
        
        Returns:
            Dict contenant le rÃ©sultat de l'action
        """
        pass
    
    @abstractmethod
    def get_prompt_context(self) -> str:
        """
        Retourne le contexte Ã  injecter dans le prompt LLM
        DÃ©crit les capabilities du plugin Ã  l'IA
        """
        pass
    
    async def on_load(self) -> None:
        """Hook appelÃ© au chargement du plugin"""
        pass
    
    async def on_unload(self) -> None:
        """Hook appelÃ© au dÃ©chargement du plugin"""
        pass
```

#### 4. Exemple concret : Plugin mÃ©tÃ©o

```python
# plugins/weather/handler.py
import httpx
from core.plugin import Plugin

class WeatherPlugin(Plugin):
    async def execute(self, intent: str, params: Dict) -> Dict:
        if intent == "get_weather":
            location = params.get("location", self.config["default_location"])
            
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    "https://api.openweathermap.org/data/2.5/weather",
                    params={
                        "q": location,
                        "appid": self.config["api_key"],
                        "units": "metric",
                        "lang": "fr"
                    }
                )
                data = response.json()
                
                return {
                    "success": True,
                    "temperature": data["main"]["temp"],
                    "feels_like": data["main"]["feels_like"],
                    "description": data["weather"][0]["description"],
                    "humidity": data["main"]["humidity"],
                    "location": location
                }
        
        elif intent == "get_forecast":
            # ImplÃ©mentation forecast 5 jours
            pass
    
    def get_prompt_context(self) -> str:
        return """Tu as accÃ¨s Ã  la fonction 'get_weather' pour obtenir la mÃ©tÃ©o actuelle.
        ParamÃ¨tres :
        - location (optionnel) : ville, dÃ©faut = Belfort, FR
        
        Retourne : tempÃ©rature, ressenti, description, humiditÃ©
        
        Exemple d'appel : {"intent": "get_weather", "params": {"location": "Paris"}}
        """
```

### Chargement dynamique des plugins

Le serveur scanne `plugins/` au dÃ©marrage et hot-reload si fichier modifiÃ© :
```python
# core/plugin_loader.py
import importlib
import json
from pathlib import Path
from typing import Dict
from core.plugin import Plugin

class PluginLoader:
    def __init__(self, plugins_dir: Path):
        self.plugins_dir = plugins_dir
        self.loaded_plugins: Dict[str, Plugin] = {}
    
    async def load_all(self):
        """Charge tous les plugins depuis plugins/"""
        for plugin_dir in self.plugins_dir.iterdir():
            if plugin_dir.is_dir() and not plugin_dir.name.startswith("_"):
                await self.load_plugin(plugin_dir.name)
    
    async def load_plugin(self, name: str):
        """Charge un plugin spÃ©cifique"""
        plugin_path = self.plugins_dir / name
        manifest_path = plugin_path / "plugin.json"
        
        with open(manifest_path) as f:
            manifest = json.load(f)
        
        if not manifest.get("enabled", True):
            return
        
        # Import dynamique
        module = importlib.import_module(f"plugins.{name}.handler")
        plugin_class = getattr(module, f"{name.title()}Plugin")
        
        # Instanciation
        config = self._load_config(manifest)
        plugin_instance = plugin_class(config)
        await plugin_instance.on_load()
        
        self.loaded_plugins[name] = plugin_instance
```

---

## ğŸ”„ Protocole de communication WebSocket

### Format des messages (JSON)

#### Client â†’ Serveur

**1. Input vocal (transcription)**
```json
{
  "type": "voice_input",
  "transcription": "Quelle est la mÃ©tÃ©o Ã  Belfort ?",
  "timestamp": "2025-01-06T14:30:00Z",
  "client_id": "desktop-pc-001",
  "language": "fr"
}
```

**2. Heartbeat**
```json
{
  "type": "ping",
  "client_id": "desktop-pc-001",
  "state": "connected",  // ou "degraded"
  "timestamp": "2025-01-06T14:30:05Z"
}
```

**3. Confirmation d'action**
```json
{
  "type": "action_confirm",
  "action_id": "calendar_add_12345",
  "status": "success",
  "client_id": "desktop-pc-001"
}
```

#### Serveur â†’ Client

**1. RÃ©ponse streaming (chunks)**
```json
{
  "type": "response_chunk",
  "content": "Il fait actuellement 8Â°C Ã  Belfort",
  "is_final": false,
  "chunk_index": 0
}
```

```json
{
  "type": "response_chunk",
  "content": " avec un ciel nuageux et une humiditÃ© de 75%.",
  "is_final": true,
  "chunk_index": 1
}
```

**2. Demande d'action (plugin)**
```json
{
  "type": "action",
  "action_id": "calendar_add_12345",
  "plugin": "calendar",
  "action": "add_reminder",
  "params": {
    "title": "Appeler le dentiste",
    "datetime": "2025-01-08T10:00:00Z",
    "description": "Prendre RDV dÃ©tartrage"
  },
  "response_text": "J'ai ajoutÃ© un rappel pour appeler le dentiste le 8 janvier Ã  10h."
}
```

**3. Notification d'erreur**
```json
{
  "type": "error",
  "code": "PLUGIN_UNAVAILABLE",
  "message": "Le plugin mÃ©tÃ©o est temporairement indisponible",
  "recoverable": true
}
```

**4. Pong (heartbeat response)**
```json
{
  "type": "pong",
  "timestamp": "2025-01-06T14:30:05Z"
}
```

### Flow de communication typique

```
CLIENT                          SERVEUR
  â”‚                                â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€ connect â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
  â”‚<â”€â”€â”€â”€â”€â”€ connected â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                                â”‚
  â”‚â”€â”€ voice_input: "mÃ©tÃ©o?" â”€â”€â”€â”€â”€â”€>â”‚
  â”‚                                â”‚â”€â”€> LLM inference
  â”‚                                â”‚â”€â”€> DÃ©tecte intent: get_weather
  â”‚                                â”‚â”€â”€> Appel plugin weather
  â”‚<â”€ response_chunk (streaming) â”€â”€â”‚
  â”‚<â”€ response_chunk (final) â”€â”€â”€â”€â”€â”€â”‚
  â”‚                                â”‚
  â”‚â”€â”€â”€â”€ ping (every 30s) â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
  â”‚<â”€â”€â”€ pong â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
```

---

## ğŸ® Commandes locales (Mode dÃ©gradÃ©)

### Liste des commandes offline

Le client Rust doit gÃ©rer ces commandes sans connexion serveur :

| Commande | Exemples de dÃ©clenchement | RÃ©ponse |
|----------|---------------------------|---------|
| **Heure** | "Quelle heure est-il ?", "Il est quelle heure ?" | "Il est 14h30" |
| **Date** | "On est quel jour ?", "Quelle est la date ?" | "Nous sommes le mardi 6 janvier 2026" |
| **ChronomÃ¨tre** | "Lance un chrono", "DÃ©marre le chronomÃ¨tre" | DÃ©marre + affichage temps Ã©coulÃ© |
| **Stop chrono** | "Stop le chrono", "ArrÃªte" | "ChronomÃ¨tre arrÃªtÃ© Ã  2 minutes 34 secondes" |
| **Minuteur** | "Mets un minuteur de 5 minutes", "Timer 10 minutes" | Compte Ã  rebours + alerte |
| **Annuler minuteur** | "Annule le minuteur", "Stop timer" | "Minuteur annulÃ©" |

### DÃ©tection de pattern (Regex simplifiÃ©s)

```rust
// Exemples de patterns Ã  implÃ©menter
const PATTERNS: &[(&str, Command)] = &[
    (r"(?i)(quelle heure|il est quelle heure)", Command::GetTime),
    (r"(?i)(quel jour|quelle.*date)", Command::GetDate),
    (r"(?i)(lance|dÃ©marre|start).*chrono", Command::StartChrono),
    (r"(?i)(stop|arrÃªte).*chrono", Command::StopChrono),
    (r"(?i)(minuteur|timer).*(\d+)\s*(minute|min)", Command::SetTimer),
];
```

### Indicateurs visuels

**Ã‰tats du client** (reflÃ©tÃ©s dans les "yeux") :
- ğŸŸ¢ **Blanc** : ConnectÃ© au serveur, prÃªt
- ğŸŸ¡ **Jaune** : Mode dÃ©gradÃ© (serveur indisponible)
- ğŸ”µ **Bleu pulsÃ©** : En train d'Ã©couter
- ğŸŸ£ **Violet** : Traitement en cours (STT ou attente rÃ©ponse)
- ğŸ”´ **Rouge** : Erreur critique

---

## ğŸ“… Plan de dÃ©veloppement

### Phase 1 : MVP Fonctionnel (2-3 semaines)

#### Semaine 1 : Serveur
- [ ] Setup Docker Compose (Python + Redis)
- [ ] FastAPI skeleton + WebSocket handler
- [ ] IntÃ©gration llama-cpp-python + tÃ©lÃ©chargement Phi-3-mini
- [ ] Context manager basique (Redis)
- [ ] Plugin loader + plugin mÃ©tÃ©o (OpenWeatherMap)
- [ ] Tests unitaires core + plugin

**Livrable** : Serveur rÃ©pond via WebSocket Ã  des inputs texte simulÃ©s

#### Semaine 2 : Client Rust (partie 1)
- [ ] Setup projet Cargo + dÃ©pendances
- [ ] Audio pipeline : capture micro (cpal)
- [ ] IntÃ©gration Whisper (modÃ¨le base)
- [ ] WebSocket client (tokio-tungstenite)
- [ ] State machine (connected/degraded)
- [ ] Commandes locales (time, date, chrono, timer)

**Livrable** : Client capture voix, transcrit, envoie au serveur

#### Semaine 3 : Client Rust (partie 2) + IntÃ©gration
- [ ] TTS basique (espeak)
- [ ] UI desktop egui (fenÃªtre + yeux)
- [ ] Gestion reconnexion automatique
- [ ] Tests end-to-end complets
- [ ] Documentation dÃ©ploiement

**Livrable** : SystÃ¨me fonctionnel bout-en-bout (vocal â†’ rÃ©ponse vocale)

---

### Phase 2 : Enrichissement (2 semaines)

#### Semaine 4 : Features avancÃ©es
- [ ] Plugin Google Calendar (OAuth2)
- [ ] Wake word detection (Porcupine)
- [ ] AmÃ©lioration TTS (migration vers piper)
- [ ] Logging structurÃ© (tracing)
- [ ] MÃ©triques performance (latence, uptime)

#### Semaine 5 : Polish & Optimisation
- [ ] Tests de charge (multiple clients)
- [ ] Optimisation latence LLM
- [ ] Interface configuration (UI settings)
- [ ] Documentation utilisateur finale

---

### Phase 3 : EmbarquÃ© (futur)

- [ ] Port client pour ESP32
- [ ] Port client pour Raspberry Pi
- [ ] IntÃ©gration Ã©cran physique (e-ink ou LCD)
- [ ] BoÃ®tier robot imprimÃ© 3D
- [ ] Alimentation batterie + gestion Ã©nergie
- [ ] Nouvelles intÃ©grations (YouTube, Spotify, etc.)

---

## ğŸ”§ Configuration requise

### Serveur (Proxmox VM/Container)
- **CPU** : 4 cores minimum (8 threads recommandÃ© pour Phi-3)
- **RAM** : 8 GB minimum (6 GB pour modÃ¨le + 2 GB OS/services)
- **Stockage** : 10 GB (modÃ¨les + logs + Redis)
- **OS** : Ubuntu 22.04 LTS (Docker)

### Client Desktop (Phase 1)
- **OS** : Windows 10+ ou Linux (Ubuntu 22.04+)
- **RAM** : 2 GB minimum
- **Audio** : Microphone + haut-parleurs/casque
- **Stockage** : 200 MB (binaire + modÃ¨les)

### Client EmbarquÃ© (Phase 3)
- **Hardware** : ESP32 (4MB RAM) ou Raspberry Pi Zero 2 W
- **Audio** : Module I2S (INMP441 mic + MAX98357A amp)
- **Display** : e-ink 2.13" ou LCD 128x64
- **Alimentation** : 5V 2A minimum

---

## ğŸ” SÃ©curitÃ© & Vie privÃ©e

### Principes
- **DonnÃ©es vocales** : Jamais stockÃ©es, traitement en mÃ©moire uniquement
- **Contexte** : ChiffrÃ© au repos dans Redis (TLS)
- **APIs tierces** : Credentials en variables d'environnement
- **RÃ©seau** : Phase 1 = LAN only (pas d'exposition Internet)

### Authentification (Phase 2+)
- Token JWT pour authentifier les clients
- Rotation automatique tous les 7 jours

---

## ğŸ“Š MÃ©triques & Monitoring

### KPIs Ã  tracker
- **Latence end-to-end** : Temps entre fin de parole et dÃ©but rÃ©ponse
- **Uptime serveur** : DisponibilitÃ© du Hive Mind
- **Taux de reconnexion** : FrÃ©quence passage mode dÃ©gradÃ©
- **PrÃ©cision STT** : Word Error Rate (manuel sampling)
- **Usage plugins** : FrÃ©quence appels par plugin

### Outils
- Logs structurÃ©s (JSON) vers stdout
- Optionnel : Grafana + Prometheus (Phase 2)

---

## ğŸ¤ Contribution & Extension

### Ajouter un nouveau plugin

1. Copier le template : `cp -r plugins/_template plugins/mon_plugin`
2. Ã‰diter `plugin.json` (name, triggers, config)
3. ImplÃ©menter `handler.py` (hÃ©riter de `Plugin`)
4. Ajouter tests dans `tests/test_mon_plugin.py`
5. Restart serveur (hot-reload automatique)

### Adapter pour nouveau matÃ©riel

1. CrÃ©er module dans `client/src/platform/nouveau.rs`
2. ImplÃ©menter trait `Platform` :
   - `init_audio()` : Config I2S/ALSA
   - `init_display()` : Config Ã©cran
   - `power_management()` : Gestion veille
3. Compiler avec feature flag : `cargo build --features=nouveau`

---

## ğŸ“š Ressources & Documentation

### ModÃ¨les IA
- [Phi-3 sur Hugging Face](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf)
- [llama.cpp documentation](https://github.com/ggerganov/llama.cpp)

### Audio
- [Whisper modÃ¨les](https://github.com/openai/whisper#available-models-and-languages)
- [Piper TTS](https://github.com/rhasspy/piper)
- [Picovoice Porcupine](https://picovoice.ai/platform/porcupine/)

### APIs
- [OpenWeatherMap API](https://openweathermap.org/api)
- [Google Calendar API](https://developers.google.com/calendar/api/guides/overview)

---

## ğŸ› Troubleshooting commun

### Serveur ne dÃ©marre pas
- VÃ©rifier RAM disponible : `free -h` (besoin 6GB+)
- VÃ©rifier modÃ¨le tÃ©lÃ©chargÃ© : `ls -lh models/`
- Logs Docker : `docker-compose logs -f`

### Client ne se connecte pas
- VÃ©rifier serveur accessible : `telnet <server_ip> 8000`
- VÃ©rifier WebSocket endpoint dans config client
- Logs client : niveau `RUST_LOG=debug`

### Latence Ã©levÃ©e (>3s)
- VÃ©rifier CPU usage serveur : `htop`
- RÃ©duire `n_ctx` dans config LLM (4096 â†’ 2048)
- VÃ©rifier pas de swap : `swapon --show`

### STT imprÃ©cis
- VÃ©rifier qualitÃ© micro (SNR > 20dB)
- Augmenter modÃ¨le Whisper (base â†’ small)
- Ajuster paramÃ¨tres VAD (Voice Activity Detection)

---

## ğŸ“ Notes de design importantes

### Pourquoi Python pour le serveur ?
- Ã‰cosystÃ¨me IA mature (transformers, llama-cpp-python)
- ItÃ©ration rapide sur prompts et plugins
- Bindings performants vers C++ (llama.cpp)

### Pourquoi Rust pour les clients ?
- Binaire unique, zÃ©ro dÃ©pendances runtime
- Performance critique pour STT/TTS en temps rÃ©el
- Ã‰cosystÃ¨me embarquÃ© mature (embedded-hal)
- Safety garanties (pas de crashes alÃ©atoires)

### Pourquoi Redis et pas SQLite ?
- Persistence + performance pour contexte partagÃ©
- Pub/Sub natif (futur : broadcast entre clients)
- TTL automatique (nettoyage contexte)
- ScalabilitÃ© (futur : multiple serveurs)

### Pourquoi Phi-3 et pas Llama/Mistral ?
- OptimisÃ© spÃ©cifiquement pour edge computing
- Meilleur instruction-following Ã  taille Ã©quivalente
- Support natif llama.cpp (GGUF)
- Latence CPU mesurÃ©e plus faible

---

## ğŸ¯ Checklist dÃ©marrage projet

### Avant de commencer
- [ ] Proxmox opÃ©rationnel (VM ou LXC prÃ©parÃ©e)
- [ ] Docker + Docker Compose installÃ©s
- [ ] Rust toolchain installÃ© (rustup)
- [ ] Compte OpenWeatherMap (API key gratuite)
- [ ] Credentials Google Cloud (Calendar API)

### Fichiers Ã  crÃ©er en prioritÃ©
1. `docker-compose.yml` (serveur + Redis)
2. `serveur/requirements.txt`
3. `serveur/main.py` (skeleton FastAPI)
4. `client/Cargo.toml`
5. `client/src/main.rs` (skeleton)

### PremiÃ¨re validation
- [ ] Serveur rÃ©pond sur `http://localhost:8000/health`
- [ ] WebSocket accepte connexion : `ws://localhost:8000/ws`
- [ ] Client compile : `cargo build --release`
- [ ] Whisper transcrit audio : test avec fichier WAV

---

## ğŸš€ Commande de dÃ©marrage rapide

### Serveur (Docker)
```bash
cd serveur/
docker-compose up -d
docker-compose logs -f  # Voir les logs
```

### Client (Rust)
```bash
cd client/
cargo run --release
```

### TÃ©lÃ©charger modÃ¨les
```bash
# Serveur : Phi-3-mini
wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf \
  -O serveur/models/phi3-mini-q4.gguf

# Client : Whisper base
wget https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.bin \
  -O client/assets/whisper-base.bin
```

---

## ğŸ“ Contact & Support

**DÃ©veloppeur principal** : Alexandre  
**Contexte** : Projet ALISON++ / SINERGIES Lab (UTBM)  
**Localisation** : Belfort, France

---

## ğŸ“„ License

Ã€ dÃ©finir (usage personnel / recherche acadÃ©mique pour l'instant)

---

**DerniÃ¨re mise Ã  jour** : 6 janvier 2026  
**Version du document** : 1.0.0
